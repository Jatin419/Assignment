{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c783fe2-5ec5-4112-ba03-f1151a2c8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans1.\n",
    "R-squared (R²) is a statistical measure used in linear regression analysis to assess the goodness of fit of a regression model. It indicates the proportion of the variance in the dependent variable that can be explained by the independent variables included in the model. In other words, it measures how well the regression model predicts the dependent variable based on the independent variables.\n",
    "\n",
    "To calculate the R-squared in linear regression models.\n",
    "the formula is R² = 1 - (RSS / TSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cede3a95-8d30-444e-90d7-e349450591e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '²' (U+00B2) (1811826738.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '²' (U+00B2)\n"
     ]
    }
   ],
   "source": [
    "#Ans2.\n",
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of predictors (independent variables) in the regression model. While regular R-squared tends to increase with the addition of more predictors, adjusted R-squared considers the complexity of the model by penalizing the inclusion of unnecessary variables.\n",
    "\n",
    "The formula for adjusted R-squared is as follows:\n",
    "\n",
    "Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\n",
    "\n",
    "The difference between adjusted R-squared and regular R-squared lies in the penalty term. The penalty term is derived from the number of predictors and the sample size, adjusting for the degrees of freedom in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a1f43-fd33-490f-b1ee-7b7b8a3e9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans3.\n",
    "Adjusted R-squared is more appropriate to use in situations where you want to compare and evaluate the performance of regression models with different numbers of predictors (independent variables) or when you want to assess the overall explanatory power of the model while considering its complexity.\n",
    "\n",
    "1.Model comparison\n",
    "2.Variable selection\n",
    "3.Model assessmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc700c-fe24-4145-986d-b4543b8b8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans5.\n",
    "RMSE, MSE, and MAE are commonly used metrics in regression analysis to evaluate the performance of a regression model and measure the accuracy of its predictions.\n",
    "\n",
    "RMSE\n",
    "\n",
    "Advantages\n",
    "1.Penalizes large errors more than MAE.\n",
    "Has the same units as the dependent variable, making it easier to interpret.\n",
    " easier to use in some machine learning algorithms.\n",
    "\n",
    "Disadvantages\n",
    "1.Sensitive to outliers.\n",
    "Can be difficult to interpret if the dependent variable has a wide range of values.\n",
    "\n",
    "MSE\n",
    "\n",
    "Advantages\n",
    "1.Penalizes large errors more than MAE.\n",
    "Is easy to calculate and interpret.\n",
    "\n",
    "Disadvantages\n",
    "1.Sensitive to outliers.\n",
    "Can be difficult to interpret if the dependent variable has a wide range of values.\n",
    "\n",
    "MAE\n",
    "\n",
    "Advantages:\n",
    "1.Not sensitive to outliers.\n",
    "2.Easy to interpret.\n",
    "\n",
    "Disadvantages\n",
    "1.Does not penalize large errors as much as RMSE or MSE.\n",
    "Is not differentiable, which makes it more difficult to use in some machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4cf5d1-c54d-4023-a058-56e9b6572b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans6.\n",
    "Lasso regularization is a technique used in linear regression to reduce the complexity of the model by shrinking the coefficients of the independent variables. This is done by adding a penalty term to the loss function that penalizes large coefficients. The penalty term is typically proportional to the absolute value of the coefficients, which means that Lasso regularization encourages some of the coefficients to be zero.\n",
    "\n",
    "Ridge regularization is another technique used in linear regression to reduce the complexity of the model. However, unlike Lasso regularization, ridge regularization does not encourage any of the coefficients to be zero.\n",
    "\n",
    "Lasso regularization is more appropriate to use when you want to reduce the complexity of the model and also want to perform feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e24c3c-05d0-4c17-ab3f-c9cb558f7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans7.\n",
    "Regularized linear models help to prevent overfitting by adding a penalty to the loss function that penalizes large coefficients.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
