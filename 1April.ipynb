{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f567b-c220-4461-9431-6cfa0b2259b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans1.\n",
    "Linear regression and logistic regression are both supervised learning algorithms that are used to predict a continuous or categorical output variable, respectively. \n",
    "\n",
    "1.Linear regression: Linear regression is used to predict a continuous output variable, such as the price of a house or the number of sales made. The model assumes that there is a linear relationship between the independent variables and the dependent variable.\n",
    "2.Logistic regression: Logistic regression is used to predict a categorical output variable, such as whether a customer will click on an ad or whether a patient has cancer. The model assumes that the relationship between the independent variables and the dependent variable is non-linear.\n",
    "\n",
    "Example\n",
    "An example of a scenario where logistic regression would be more appropriate:\n",
    "\n",
    "--> Predicting whether a customer will click on an ad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05fa72-4ed1-4d2d-a68c-588ce2e6aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans2.\n",
    "The cost function used in logistic regression is called the \"log loss\" or \"binary cross-entropy\" cost function. \n",
    "The cross-entropy loss is minimized using an optimization algorithm, such as gradient descent.\n",
    "\n",
    "loss = -∑(y * log(hθ(x)) + (1 - y) * log(1 - hθ(x)))\n",
    "\n",
    "The cross-entropy loss is a convex function, which means that it has a single global minimum. This makes it possible to use gradient descent to efficiently minimize the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e59fe9d-87c6-4074-a1cf-befc45a183d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 6) (1831936050.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    L1 regularization adds a penalty to the sum of the absolute values of the model's parameters. This encourages the model to set some of its parameters to zero, which can help to reduce the model's complexity. L2 regularization adds a penalty to the sum of the squared values of the model's parameters. This encourages the model to shrink all of its parameters, but it does not force any of them to be zero.\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 6)\n"
     ]
    }
   ],
   "source": [
    "#Ans3.\n",
    "Regularization is a technique used to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data too well and is unable to generalize to new data. \n",
    "\n",
    "There are two main types of regularization: \n",
    "L1 and L2.\n",
    "L1 regularization adds a penalty to the sum of the absolute values of the model's parameters. This encourages the model to set some of its parameters to zero, which can help to reduce the model's complexity. L2 regularization adds a penalty to the sum of the squared values of the model's parameters. This encourages the model to shrink all of its parameters, but it does not force any of them to be zero.\n",
    "\n",
    "An example of how regularization can help prevent overfitting in logistic regression. Let's say we have a dataset of 1000 examples, each with two features. We train a logistic regression model on this dataset and find that the model has a training accuracy of 99%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593278a-f851-4038-b14f-329771ebf5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans4.\n",
    "The ROC curve, or Receiver Operating Characteristic curve, is a graphical plot that summarizes the performance of a binary classifier.\n",
    "\n",
    "The TPR is the ratio of true positives to the total number of positives, while the FPR is the ratio of false positives to the total number of negatives. A perfect classifier would have a TPR of 1 and an FPR of 0, while a random classifier would have a TPR and FPR of 0.5.\n",
    "The ROC curve is a useful tool for evaluating the performance of a logistic regression model because it allows us to see how the model performs at different thresholds. This is important because the choice of threshold can have a significant impact on the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84814207-d0d3-40b8-b353-fb007fb16b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans5.\n",
    "Feature selection techniques in logistic regression aim to identify and select the most relevant and informative features to improve the model's performance\n",
    "\n",
    "Techniques help improve the model's performance are :\n",
    "1.Filter methods: These methods select features based on their statistical significance. \n",
    "2.L1 Regularization (Lasso):\n",
    "  L1 regularization, as discussed earlier, introduces a penalty term based on the absolute values of the model's coefficients.\n",
    "3.Improve Efficiency: Removing redundant or irrelevant features can lead to a more efficient model in terms of computation time and memory requirements, especially when dealing with high-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89d320-f8b1-4274-9dfa-5be8078ed166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans6.\n",
    "Handling imbalanced datasets in logistic regression is an important consideration as it can lead to biased models with poor performance on the minority class.\n",
    "\n",
    "-->Oversampling: Oversampling is a simple and effective way to balance an imbalanced dataset. However, it can also lead to overfitting, so it is important to use it carefully.\n",
    "-->Undersampling: Undersampling is another simple way to balance an imbalanced dataset. However, it can also lead to underfitting, so it is important to use it carefully.\n",
    "-->Cost-sensitive learning: Cost-sensitive learning is a more sophisticated approach to dealing with class imbalance. \n",
    "-->Ensemble learning: Ensemble learning is a powerful technique that can be used to improve the performance of machine learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac060dc-c152-4105-8c8b-b4f7c5f8805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans7.\n",
    "Some common issues and challenges that may arise when implementing logistic regression:\n",
    "    \n",
    "    Overfitting: Logistic regression is a relatively simple model, so it is susceptible to overfitting. This can happen if the model is trained on a dataset that is too small or if the model has too many parameters.\n",
    "    Underfitting: Logistic regression can also underfit the data if the model is not complex enough. This can happen if the model is trained on a dataset that is too large or if the model does not have enough parameters.\n",
    "    Non-linearity: Logistic regression assumes that the relationship between the independent variables and the dependent variable is linear.\n",
    "\n",
    "Some strategies for addressing these issues:\n",
    "    Overfitting: To address overfitting, you can use regularization techniques, such as L1 or L2 regularization. You can also try to reduce the size of the model by removing some of the independent variables.\n",
    "    Underfitting: To address underfitting, you can try to increase the complexity of the model by adding more independent variables or by using a more complex model, such as a neural network.\n",
    "    Multicollinearity: To address multicollinearity, you can try to remove one of the correlated variables from the model. You can also try to combine the correlated variables into a single variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
