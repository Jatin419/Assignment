{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93261239-d91c-42fd-9b50-5cbec5086328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans1.\n",
    "The k-nearest neighbors algorithm (KNN) is a non-parametric, supervised learning algorithm that can be used for both classification and regression problems. It works by finding the k most similar instances in the training set to a new instance, and then predicting the class label or target value of the new instance based on the class labels of its k nearest neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa368564-858e-4a6c-a395-437e1e2f739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans2.\n",
    "Choosing the value of K in the K-Nearest Neighbors (KNN) algorithm is a critical step that can significantly impact the model's performance.\n",
    "\n",
    "Some common methods to choose the value of K:\n",
    "    1.Cross-Validation\n",
    "    2.Odd Values of K\n",
    "    3.Square Root Rule\n",
    "    4.Domain Knowledge\n",
    "    5.Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647fff7-4a44-4ffa-88ef-8c2db8f80873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans3.\n",
    "\n",
    "The main difference between KNN classifier and KNN regressor is the type of output they produce. \n",
    "\n",
    "KNN Classifier:\n",
    "    1.Problem Type: The KNN classifier is used for classification tasks, where the goal is to assign a discrete class label to a new data point based on its features.\n",
    "    2.Output: The output of the KNN classifier is a class label, representing the predicted class for the new data point.\n",
    "    \n",
    "KNN Regressor:\n",
    "    1.Problem Type: The KNN regressor is used for regression tasks, where the goal is to predict a continuous target value for a new data point based on its features.\n",
    "    2.Output: The output of the KNN regressor is a numerical value, representing the predicted target value for the new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b9ac44-fa07-428c-993e-222f0be886b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans4.\n",
    "To measure the performance of the K-Nearest Neighbors (KNN) algorithm, you can use various evaluation metrics depending on the type of problem you are solving (classification or regression).\n",
    "\n",
    "Accuracy: This is the most common metric for measuring the performance of a classification algorithm.\n",
    "Precision: This metric measures the fraction of predicted positive instances that are actually positive. \n",
    "Precision: This metric measures the fraction of predicted positive instances that are actually positive. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8fd20f-43e9-4842-8b64-72cf23958fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans5.\n",
    "The \"Curse of Dimensionality\" is a term used to describe the adverse effects of high-dimensional data on various machine learning algorithms, including the K-Nearest Neighbors (KNN) algorithm.\n",
    "There are a few reasons why the curse of dimensionality affects KNN. First, as the number of features increases, the volume of the data space grows exponentially. This means that the dataset must also grow exponentially in order to keep the same density. If the dataset does not grow exponentially, then the points in the dataset will become more spread out, and it will be more difficult for KNN to find similar points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651eea4-2acf-4025-a9a5-bf06d6ef3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans6.\n",
    "\n",
    "There are a few different ways to handle missing values in KNN. One way is to simply ignore the instances with missing values. This is the simplest approach, but it can lead to decreased accuracy.\n",
    "Another way to handle missing values is to impute them. Imputation is the process of filling in missing values with estimates. There are a number of different imputation techniques that can be used, such as mean imputation, median imputation, and k-nearest neighbors imputation.\n",
    "\n",
    "several strategies to handle missing values in KNN:\n",
    "    1.Imputation\n",
    "    2.Missingness Indicators\n",
    "    3.Multiple Imputation\n",
    "    4.KNN Imputation\n",
    "    5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf384bfb-90c4-449f-80c0-3dfb2e7ba674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans7.\n",
    "The performance of the K-Nearest Neighbors (KNN) classifier and regressor can vary depending on the type of problem and the characteristics of the data. \n",
    "\n",
    "KNN Classifier:\n",
    "\n",
    "Problem Type: The KNN classifier is used for classification tasks, where the goal is to predict the class label of a new data point based on its features.\n",
    "Output: The output of the KNN classifier is a discrete class label.\n",
    "Performance Evaluation: Classification performance is typically evaluated using metrics like accuracy, precision, recall, F1-score, ROC curve, and AUC-ROC.\n",
    "\n",
    "KNN Regressor:\n",
    "\n",
    "Problem Type: The KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value for a new data point based on its features.\n",
    "Output: The output of the KNN regressor is a continuous numerical value.\n",
    "Performance Evaluation: Regression performance is typically evaluated using metrics like mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared.\n",
    "\n",
    ", KNN classifiers are suitable for discrete class prediction problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23840998-18c1-49d5-bebb-cfe33a8b5e60",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (133943817.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    Strengths of KNN Algorithm:\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Ans8.\n",
    "Strengths of KNN Algorithm:\n",
    "    1.Simple and Intuitive: KNN is easy to understand and implement, making it an excellent choice for quick prototyping and baseline models.\n",
    "    2.Adaptability to Data:KNN is a non-parametric algorithm, meaning it makes no assumptions about the underlying data distribution.\n",
    "    3.Interpretability: KNNs decision-making process is transparent and can be easily understood, making it useful for gaining insights into the data.\n",
    "\n",
    "Weaknesses of KNN Algorithm:\n",
    "\n",
    "    1.Computationally Expensive: KNN's main drawback is its computational complexity, as it requires calculating distances between the query point and all training samples.\n",
    "    2.Imbalanced Data: KNN treats all neighbors equally, so it may be biased toward the majority class in imbalanced datasets.\n",
    "    3.Sensitive to Noise and Outliers: KNN can be affected by noisy data and outliers, which can lead to erroneous predictions.\n",
    "\n",
    "Addressing KNN Algorithm's Weaknesses:\n",
    "1.Distance Metric Optimization\n",
    "2.Dimensionality Reduction\n",
    "3.Data Preprocessing\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665444e-fdcf-41e0-85a6-c56bd89cac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans9.\n",
    "uclidean distance and Manhattan distance are two of the most common distance metrics used in KNN. They are both used to measure the distance between two points in a multidimensional space. However, they differ in the way that they calculate the distance.\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points. It is calculated by taking the square root of the sum of the squared differences between the corresponding features of the two points. \n",
    "\n",
    "The formula for Euclidean distance is:\n",
    "    distance = sqrt(sum((x1 - x2)^2))\n",
    "\n",
    "Manhattan distance is the sum of the absolute differences between the corresponding features of the two points. The formula for Manhattan distance is:\n",
    "\n",
    "distance = sum(|x1 - x2|)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0e390-fdd0-4656-82de-c91882603734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans10.\n",
    "\n",
    "Feature scaling is the process of normalizing the range of features in a dataset. This is done to ensure that all features have a similar scale, which can help to improve the performance of machine learning algorithms.\n",
    "\n",
    "In KNN, feature scaling is important because it can help to prevent features with larger magnitudes from dominating the distance calculations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
