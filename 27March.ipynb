{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c783fe2-5ec5-4112-ba03-f1151a2c8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans1.\n",
    "R-squared (R²) is a statistical measure used in linear regression analysis to assess the goodness of fit of a regression model. It indicates the proportion of the variance in the dependent variable that can be explained by the independent variables included in the model. In other words, it measures how well the regression model predicts the dependent variable based on the independent variables.\n",
    "\n",
    "To calculate the R-squared in linear regression models.\n",
    "the formula is R² = 1 - (RSS / TSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbc574-f0a7-4a9d-93df-b018677008e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans4.\n",
    "MSE, MSE, and MAE are all metrics used to evaluate the accuracy of a regression model. They are all calculated by comparing the predicted values of the model to the actual values.\n",
    "\n",
    "Mean Absolute Error (MAE) is the average of the absolute errors between the predicted and actual values. This means that it ignores the direction of the error (whether the predicted value is too high or too low) and simply measures the distance between the two values.\n",
    "Mean Squared Error (MSE) is the average of the squared errors between the predicted and actual values. This means that it gives more weight to larger errors, and can be more sensitive to outliers than MAE.\n",
    "Root Mean Squared Error (RMSE) is the square root of the MSE. This means that it has the same units as the predicted values, and is often easier to interpret than the MSE.\n",
    "\n",
    "a lower value of RMSE, MSE, or MAE indicates a more accurate regression model. However, the best metric to use depends on the specific situation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c7201-a3f1-4d45-bc59-fec7090592eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans8.\n",
    "\n",
    "Regularized linear models are a type of machine learning algorithm that can be used to solve regression problems. They work by adding a penalty to the loss function, which helps to prevent the model from overfitting the training data. \n",
    "\n",
    "regularized linear models can be computationally expensive to train. \n",
    "regularized linear models are a powerful tool for regression analysis. However, they are not always the best choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae52b4-ae40-48a2-b5de-e6e78d74ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans9.\n",
    "a lower value of RMSE or MAE indicates a more accurate regression model. However, the best metric to use depends on the specific situation. \n",
    "\n",
    "Model A has a lower RMSE than Model B. This means that Model A's predictions are on average closer to the actual values than Model B's predictions. Therefore, I would choose Model A as the better performer.\n",
    "\n",
    " the best way to choose between two regression models is to consider all of the available metrics and to use your judgment based on the specific situation.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ea361-4311-446e-894a-94cf240ac373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans10.\n",
    "Ridge regularization and Lasso regularization are both types of regularized linear models that can be used to prevent overfitting\n",
    "\n",
    "Ridge regularization adds a penalty to the sum of the squared weights of the model. \n",
    "Lasso regularization adds a penalty to the sum of the absolute values of the weights of the model.\n",
    "\n",
    "Model A uses Ridge regularization with a regularization parameter of 0.1. This means that it will penalize models with large weights, but it will not be as aggressive as Model B, which uses Lasso regularization with a regularization parameter of 0.5\n",
    "Model A is likely to be more stable than Model B. This means that it is less likely to overfit the training data. \n",
    "The best way to choose between Model A and Model B is to evaluate their performance on a held-out test set\n",
    "\n",
    "Some trade-offs and limitations to consider when choosing between Ridge and Lasso regularization:\n",
    "1.Ridge regularization is more stable than Lasso regularization. \n",
    "2.Lasso regularization can be used for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cede3a95-8d30-444e-90d7-e349450591e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '²' (U+00B2) (1811826738.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '²' (U+00B2)\n"
     ]
    }
   ],
   "source": [
    "#Ans2.\n",
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of predictors (independent variables) in the regression model. While regular R-squared tends to increase with the addition of more predictors, adjusted R-squared considers the complexity of the model by penalizing the inclusion of unnecessary variables.\n",
    "\n",
    "The formula for adjusted R-squared is as follows:\n",
    "\n",
    "Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\n",
    "\n",
    "The difference between adjusted R-squared and regular R-squared lies in the penalty term. The penalty term is derived from the number of predictors and the sample size, adjusting for the degrees of freedom in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a1f43-fd33-490f-b1ee-7b7b8a3e9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans3.\n",
    "Adjusted R-squared is more appropriate to use in situations where you want to compare and evaluate the performance of regression models with different numbers of predictors (independent variables) or when you want to assess the overall explanatory power of the model while considering its complexity.\n",
    "\n",
    "1.Model comparison\n",
    "2.Variable selection\n",
    "3.Model assessmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc700c-fe24-4145-986d-b4543b8b8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans5.\n",
    "RMSE, MSE, and MAE are commonly used metrics in regression analysis to evaluate the performance of a regression model and measure the accuracy of its predictions.\n",
    "\n",
    "RMSE\n",
    "\n",
    "Advantages\n",
    "1.Penalizes large errors more than MAE.\n",
    "Has the same units as the dependent variable, making it easier to interpret.\n",
    " easier to use in some machine learning algorithms.\n",
    "\n",
    "Disadvantages\n",
    "1.Sensitive to outliers.\n",
    "Can be difficult to interpret if the dependent variable has a wide range of values.\n",
    "\n",
    "MSE\n",
    "\n",
    "Advantages\n",
    "1.Penalizes large errors more than MAE.\n",
    "Is easy to calculate and interpret.\n",
    "\n",
    "Disadvantages\n",
    "1.Sensitive to outliers.\n",
    "Can be difficult to interpret if the dependent variable has a wide range of values.\n",
    "\n",
    "MAE\n",
    "\n",
    "Advantages:\n",
    "1.Not sensitive to outliers.\n",
    "2.Easy to interpret.\n",
    "\n",
    "Disadvantages\n",
    "1.Does not penalize large errors as much as RMSE or MSE.\n",
    "Is not differentiable, which makes it more difficult to use in some machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4cf5d1-c54d-4023-a058-56e9b6572b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans6.\n",
    "Lasso regularization is a technique used in linear regression to reduce the complexity of the model by shrinking the coefficients of the independent variables. This is done by adding a penalty term to the loss function that penalizes large coefficients. The penalty term is typically proportional to the absolute value of the coefficients, which means that Lasso regularization encourages some of the coefficients to be zero.\n",
    "\n",
    "Ridge regularization is another technique used in linear regression to reduce the complexity of the model. However, unlike Lasso regularization, ridge regularization does not encourage any of the coefficients to be zero.\n",
    "\n",
    "Lasso regularization is more appropriate to use when you want to reduce the complexity of the model and also want to perform feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e24c3c-05d0-4c17-ab3f-c9cb558f7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans7.\n",
    "Regularized linear models help to prevent overfitting by adding a penalty to the loss function that penalizes large coefficients.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
