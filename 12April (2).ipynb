{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d194eb0-c3d9-463c-a039-6ad055fe1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans1.\n",
    "Bagging helps to prevent decision trees from learning the training data too well. This makes the trees more generalizable to new data, which reduces overfitting.\n",
    "\n",
    "Bootstrap Resampling: In bagging, multiple subsets of the original training data are created by sampling with replacement. Each subset is of the same size as the original training set.\n",
    "\n",
    "Aggregation of Multiple Base Models: Bagging involves training multiple decision trees (base models) on the bootstrapped subsets. Each decision tree will learn different patterns from the various subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8514f-922d-4902-8917-e97946fe81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans2.\n",
    "Advantages\n",
    "Improved accuracy: Using different types of base learners can help to improve the accuracy of the ensemble model\n",
    "Reduced variance: Using different types of base learners can help to reduce the variance of the ensemble model. \n",
    "Increased robustness: Using different types of base learners can help to increase the robustness of the ensemble model.\n",
    "\n",
    "Disadvantages\n",
    "1.Increased complexity: Using different types of base learners can increase the complexity of the ensemble model.\n",
    "2.Increased computational cost: Using different types of base learners can increase the computational cost of training the ensemble model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4863d-bf0d-4db9-9f1d-68957711b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans3.\n",
    "Bias: Base learners with high bias, such as decision trees, can help to reduce variance in bagging. This is because high-bias learners are less likely to overfit the training data, which can lead to more generalizable predictions.\n",
    "\n",
    "Variance: Base learners with low variance, such as linear regression, can help to reduce bias in bagging.\n",
    "\n",
    "Tradeoff: The choice of base learner in bagging can be a trade-off between bias and variance.\n",
    "\n",
    "The best choice of base learner for bagging will depend on the specific problem at hand. If the goal is to achieve the highest possible accuracy, then a low-variance learner may be a good option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7c83c-a01e-4d6f-837c-7ed54365a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans4.\n",
    "Yes, bagging can be used for both classification and regression tasks, and the implementation differs slightly between the two cases.\n",
    "\n",
    "Bagging is used to create an ensemble of linear regression models. Each linear regression model is trained on a bootstrap sample of the training data. The predictions of the individual models are then combined to create a final prediction.\n",
    "\n",
    "The main difference between bagging for classification and regression tasks is the way that the predictions of the individual models are combined. In classification tasks, the predictions of the individual trees are combined using majority voting. This means that the final prediction is the class that is most popular among the individual trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1022bee-2a3d-4e45-9a5a-4ebc359ff3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans5.\n",
    "The ensemble size in bagging is the number of models that are included in the ensemble. The ensemble size can have a significant impact on the performance of the ensemble model.\n",
    "\n",
    "The individual models in an ensemble are trained on different bootstrap samples of the training data. This helps to prevent the ensemble model from overfitting the training data.\n",
    "\n",
    "The combination of the individual models in an ensemble can help to reduce bias. This is because the individual models may make different mistakes. As a result, the ensemble model is less likely to make the same mistakes as any of the individual models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac6bcc-653f-4e5c-80b5-e0f5b8518796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans6.\n",
    "Bagging is a popular ensemble technique that is used in a variety of real-world applications. \n",
    "\n",
    "-->Fraud detection: Bagging is used in fraud detection to identify fraudulent transactions. In this application, bagging is used to create an ensemble of decision trees\n",
    "-->Medical diagnosis: Bagging is used in medical diagnosis to identify diseases. In this application, bagging is used to create an ensemble of classifiers.\n",
    "-->Stock market prediction: Bagging is used in stock market prediction to predict stock prices. In this application, bagging is used to create an ensemble of regression models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
