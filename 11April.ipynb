{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9106b5a-c2bd-4e29-bf44-57a4b88b7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans1.\n",
    "Ensemble techniques in machine learning are methods that combine multiple models to create a more accurate and robust prediction. Ensemble techniques can be used to reduce variance, improve accuracy, and make models more robust.\n",
    "\n",
    "Some of the most common ensemble techniques include bagging, boosting, and stacking.\n",
    "\n",
    "Bagging stands for bootstrap aggregating, in which multiple copies of the training data are created by sampling with replacement. \n",
    "\n",
    "Boosting is a technique that iteratively trains a series of models, each of which is designed to correct the errors of the previous models.\n",
    "\n",
    "Stacking is a technique that combines the predictions of multiple models using a meta-model. The meta-model is a third model that is trained on the predictions of the individual models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e15c72-5c60-4583-804e-ee30256bc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans2.\n",
    "\n",
    "Ensemble techniques are used in machine learning for several compelling reasons, as they offer numerous advantages and benefits over individual models:\n",
    "    \n",
    "    -->Improved Performance: Ensemble methods often lead to better predictive performance compared to single models\n",
    "    -->To reduce variance: Ensemble techniques can help to reduce variance, which is a measure of how much a model's predictions change when the training data is changed. This can make the model more robust to changes in the data\n",
    "    -->To improve robustness: Ensemble techniques can help to make a model more robust to noise and outliers. This is because ensemble techniques can combine the predictions of multiple models, which can help to mitigate the effects of noise and outliers.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51dad0-dd0c-44eb-a1d2-9300acc91856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans3.\n",
    "Bagging, short for Bootstrap Aggregating, is an ensemble machine learning technique that aims to improve the accuracy and robustness of predictive models. The primary idea behind bagging is to create multiple base models (often the same type of model) and train each model independently on different subsets of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a6f44-2fda-414d-bcb5-90f3cb98cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans4.\n",
    "Boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b365c35-5321-4359-8485-46a788750c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans5.\n",
    "Ensemble techniques are a powerful tool that can be used to improve the performance of machine learning models. They can help to reduce variance, improve robustness, and increase interpretability. However, they can also be computationally expensive and difficult to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eec5c0-37cd-4bc5-8806-d4b833437853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans6.\n",
    "Ensemble techniques can be better than individual models, but not always. They can be more complex and computationally expensive, but can also combine the strengths of multiple models to create a more accurate and robust prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c5284-18d7-46d8-a38a-1e8f2e53198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans7.\n",
    "\n",
    "Bootstrapping is a statistical method that can be used to estimate the uncertainty of a statistic. It works by resampling the data with replacement and then calculating the statistic of interest on each of the bootstrap samples.\n",
    "\n",
    "Data Resampling: Starting with the original dataset of size \"n\", multiple (often thousands or more) resamples of the same size \"n\" are created by sampling with replacement from the original data.\n",
    "\n",
    "Statistical Calculation: For each of the resamples, the sample statistic of interest (e.g., mean, median, standard deviation, etc.) is calculated.\n",
    "\n",
    "Confidence Interval Calculation: With the distribution of the sample statistic calculated from the resamples, you can now estimate the confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8408789-7d5c-4b5a-9d80-c5902a4e4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans8.\n",
    "Bootstrapping is a statistical method that can be used to estimate the uncertainty of a statistic. It works by resampling the data with replacement and then calculating the statistic of interest on each of the bootstrap samples.\n",
    "\n",
    "The steps are:\n",
    "    \n",
    "-->Choose the statistic of interest.\n",
    "\n",
    "-->Resample the data with replacement.\n",
    "\n",
    "-->Calculate the statistic of interest on each of the bootstrap samples.\n",
    "\n",
    "-->Find the 2.5th and 97.5th percentiles of the bootstrapped distribution.\n",
    "\n",
    "-->The 2.5th and 97.5th percentiles define the lower and upper bounds of the confidence interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121db128-4d3e-4af9-beca-9ad9d5bf3f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0fe65-44b3-4e64-b011-a8d1bf7d188b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
